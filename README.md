

# PhysLab

<img src="img/pevd.png" alt="pevd" style="zoom: 45%;" />

<div align="center"><img src="https://img.shields.io/badge/Version-1.0--alpha-brightgreen"> </div>

## üåêIntroduction

The project built and opened the industry's first physical experiment process video dataset that integrates multi-modal and multi-scenario applications - PhysLab. The dataset covers multiple key research areas such as behavior recognition, target detection, instance segmentation, human-computer interaction detection, scene graph generation, etc., and has carried out comprehensive and detailed data annotation in each field.

The PEVD dataset contains 4 carefully constructed subsets, each of which supports a range of specific research areas. The specific introduction is as follows:

- AR: 620 long videos of four physical experiments are recorded, covering 3873 action clips of 32 types of actions, with an average length of 20 seconds per clip and a video frame rate of 30FPS. This subset provides valuable resources for video behavior research such as action recognition, action segmentation, and behavior prediction.

  <img src="img/Fig. 1.jpg" alt="Fig. 1" style="zoom:25%;" />

- DET: contains 4,500 experimental images, covering conventional boundary annotations of object instances, occlusion annotations, human-object interaction annotations, and scene map annotations.

  <img src="img/intro.jpg" alt="Âõæ4-1" style="zoom: 25%;" />

- SG: Contains about 3097 images, which are annotated with fine-grained instance segmentation, involving pixel-level category information of human body parts of operating students and physical experimental equipment, supporting instance segmentation, semantic segmentation and other research.

  <img src="img/Fig. 3.jpg" alt="Âõæ4-1" style="zoom: 5%;" />

- MM: Currently under development, the goal is to integrate text and image data of the experimental process to support multimodal research, including advanced visual tasks such as image description generation, text description generation, and cross-modal alignment.

  <img src="img/Fig. 2.jpg" alt="Fig. 2" style="zoom:15%;" />


## Disclaimer

The physical experiment dataset provided by this project is collected and annotated based on specific experimental scenarios and methods, but the dataset may contain a certain degree of deviation, incompleteness or erroneous information. Therefore, this dataset is for reference and research purposes only, and its absolute accuracy and applicability are not guaranteed. The results of analysis, modeling or other research activities using this dataset may contain errors or deviations and cannot be directly used for practical applications or decision-making. This project is not responsible for any consequences or losses arising from the use of the dataset. Users should bear their own risks when using the dataset and conduct necessary verification and validation of the data and research results.
